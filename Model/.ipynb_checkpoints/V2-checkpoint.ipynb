{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time \n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Step 0 : Get Data\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def GetData(data_path):\n",
    "    operation_train = pd.read_csv(data_path + \"operation_train_new.csv\")\n",
    "    transaction_train = pd.read_csv(data_path + \"transaction_train_new.csv\")\n",
    "    operation_test = pd.read_csv(data_path + \"operation_round1_new.csv\")\n",
    "    transaction_test = pd.read_csv(data_path + \"transaction_round1_new.csv\")\n",
    "    tag_train = pd.read_csv(data_path + \"tag_train_new.csv\")\n",
    "    tag_test = pd.read_csv(\"../Data/submission_example.csv\")\n",
    "    return operation_train, transaction_train, tag_train, operation_test, transaction_test, tag_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Step 1 : Data Preprocessing \n",
    "\n",
    "    Drop Duplicates\n",
    "    Fill Na\n",
    "    Drop Outliers: After Data Exploration \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def CreateTagTest(operation_test, transaction_test):\n",
    "    uids = pd.concat([operation_test[\"UID\"], transaction_test[\"UID\"]]).unique()\n",
    "    uids = np.sort(uids)\n",
    "    tag_test = pd.DataFrame()\n",
    "    tag_test[\"UID\"] = uids\n",
    "    tag_test[\"Tag\"] = -1\n",
    "    return tag_test\n",
    "\n",
    "def MergeData(operation, transaction, tag):\n",
    "    uid_tag_dict = dict(zip(tag[\"UID\"], tag[\"Tag\"]))\n",
    "    operation[\"tag\"] = operation[\"UID\"].map(uid_tag_dict)\n",
    "    transaction[\"tag\"] = transaction[\"UID\"].map(uid_tag_dict)\n",
    "    return operation, transaction\n",
    "\n",
    "def DropDuplicates(data):\n",
    "    print(\"before: \")\n",
    "    print(\"data.shape: \", data.shape)\n",
    "    data = data.drop_duplicates()\n",
    "    print(\"after: \")\n",
    "    print(\"data.shape: \", data.shape)\n",
    "    return data\n",
    "\n",
    "def DataPreprocessing(operation_train, transaction_train, tag_train, operation_test, transaction_test):\n",
    "    tag_test = CreateTagTest(operation_test, transaction_test)\n",
    "\n",
    "    operation_train, transaction_train = MergeData(operation_train, transaction_train, tag_train)\n",
    "    operation_test, transaction_test = MergeData(operation_test, transaction_test, tag_test)\n",
    "\n",
    "    operation_train = DropDuplicates(operation_train)\n",
    "    transaction_train = DropDuplicates(transaction_train)\n",
    "    operation_test = DropDuplicates(operation_test)\n",
    "    transaction_test = DropDuplicates(transaction_test)\n",
    "    return operation_train, transaction_train, tag_train, operation_test, transaction_test, tag_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Step 2 : Data Exploration\n",
    "    \n",
    "    Categorical Columns : countplot, barplot\n",
    "    Numerical Columns : regplot\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def DataExploration(operation, transaction, train_mode = False):\n",
    "    \n",
    "    print(\"Operation Countplot\")\n",
    "    plt.figure(figsize = (15, 10))\n",
    "    i = 1\n",
    "    for column in operation.columns:\n",
    "        if operation[column].unique().shape[0] < 50:\n",
    "            print(\"Plot \" + column + \"...\")\n",
    "            plt.subplot(2, 3, i)\n",
    "            sns.countplot(x = column, data = operation)\n",
    "            i += 1\n",
    "    plt.suptitle(\"Operation Countplot\")\n",
    "    plt.show()\n",
    "\n",
    "    if train_mode:\n",
    "        print(\"#\" * 100)\n",
    "        print(\"Operation Barplot\")\n",
    "        plt.figure(figsize = (15, 10))\n",
    "        i = 1\n",
    "        for column in operation.columns:\n",
    "            if operation[column].unique().shape[0] < 50 and column != \"tag\":\n",
    "                print(\"Plot \" + column + \"...\")\n",
    "                plt.subplot(2, 2, i)\n",
    "                sns.barplot(x = column, y = \"tag\", data = operation.sample(100000))\n",
    "                i += 1\n",
    "        plt.suptitle(\"Operation Barplot\")\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"#\" * 100)\n",
    "    print(\"Transaction Countplot\")\n",
    "    plt.figure(figsize = (15, 10))\n",
    "    i = 1\n",
    "    for column in transaction.columns:\n",
    "        if transaction[column].unique().shape[0] < 50:\n",
    "            print(\"Plot \" + column + \"...\")\n",
    "            plt.subplot(2, 4, i)\n",
    "            sns.countplot(x = column, data = transaction)\n",
    "            i += 1\n",
    "    plt.suptitle(\"Transaction Countplot\")\n",
    "    plt.show()\n",
    "\n",
    "    if train_mode:\n",
    "        print(\"#\" * 100)\n",
    "        print(\"Transaction Barplot\")\n",
    "        plt.figure(figsize = (15, 10))\n",
    "        i = 1\n",
    "        for column in transaction.columns:\n",
    "            if transaction[column].unique().shape[0] < 50 and column != \"tag\":\n",
    "                print(\"Plot \" + column + \"...\")\n",
    "                plt.subplot(2, 3, i)\n",
    "                sns.barplot(x = column, y = \"tag\", data = transaction.sample(100000))\n",
    "                i += 1\n",
    "        plt.suptitle(\"transaction Barplot\")\n",
    "        plt.show()\n",
    "\n",
    "# operation\n",
    "# 时间日期：day, time\n",
    "# 操作：mode, success\n",
    "# 操作系统：os\n",
    "# 版本：version\n",
    "# 设备：device1, device2, device_code1, device_code2, device_code3\n",
    "# ip：ip1, ip2, ip1_sub, ip2_sub\n",
    "# mac：mac1, mac2\n",
    "# wifi：wifi\n",
    "# 地理位置：geo_code\n",
    "\n",
    "# transaction\n",
    "# 平台：channel\n",
    "# 日期时间：day, time\n",
    "# 资金：trans_amt, bal, amt_src1, amt_src2\n",
    "# 商户：merchant, code1, code2\n",
    "# 交易类型：trans_type1, trans_typ2\n",
    "# 账户：acc_id1, acc_id2\n",
    "# 设备：device_code1, device_code2, device_code3, device1, device2\n",
    "# ip：ip1, ip1_sub\n",
    "# mac：mac1\n",
    "# 地理位置：geocode\n",
    "# 营销活动：market_code, market_type\n",
    "\n",
    "# operation countplot结论\n",
    "# 1 日期分布较为均匀，操作记录较多的日期有1，8，15，22，29\n",
    "# 2 绝大多数操作记录都成功了\n",
    "# 3 操作系统分布不均，最多的是102，最少的是101和107\n",
    "# 4 版本中，操作记录最多的是7.0.9和7.0.5\n",
    "# 5 黑白样本记录不平衡，黑记录 / 白记录 = 137371 / 1096976 = 0.125；黑白样本比例不平衡，黑样本 / 白样本 = 4285 / 26894 = 0.16\n",
    "\n",
    "# operation barplot结论\n",
    "# 1 日期分布较为均匀，27附近的黑样本比例较高\n",
    "# 2 操作成功的黑样本比例较高\n",
    "# 3 操作系统为107的样本均为白样本，105的样本均为黑样本\n",
    "# 4 版本为6.1.0，4.1.7的样本均为黑样本，6.5.0，7.0.0，6.6.3的黑样本比例较高，许多版本的样本均为白样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Step 3 : Feature Engineering\n",
    "    \n",
    "    Feature Creation\n",
    "    Feature Selection\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def FeatureCreation(data, tag):\n",
    "    data[\"hour\"] = data[\"time\"].apply(lambda x : int(x[:2]))\n",
    "    data[\"minute\"] = data[\"time\"].apply(lambda x : int(x[3:5]))\n",
    "    data[\"second\"] = data[\"time\"].apply(lambda x : int(x[6:]))\n",
    "    uid_count = data.groupby(\"UID\").count()[\"tag\"].reset_index().rename(columns = {\"tag\" : \"uid_count\"})\n",
    "    data_features = pd.merge(left = tag, right = uid_count, on = \"UID\", how = \"outer\")\n",
    "    for column in data.columns:\n",
    "        if column != \"UID\" and column != \"tag\":\n",
    "            print(\"Create \" + column + \"_nunique...\")\n",
    "            column_nunique = data.groupby(\"UID\").nunique()[column].reset_index().rename(columns = {column : column + \"_nunique\"})\n",
    "            data_features = pd.merge(left = data_features, right = column_nunique, on = \"UID\", how = \"outer\")\n",
    "            if data[column].nunique() < 50:\n",
    "                column_values = data[column].unique()\n",
    "                for value in column_values:\n",
    "                    if str(value) != \"nan\":\n",
    "                        print(\"Create \" + column + \"_\" + str(value) + \"_count...\")\n",
    "                        column_value = data[data[column] == value]\n",
    "                        column_value_count = column_value.groupby(\"UID\").count()[column].reset_index().rename(columns = {column : column + \"_\" + str(value) + \"_count\"})\n",
    "                        data_features = pd.merge(data_features, column_value_count, on = \"UID\", how = \"outer\")\n",
    "    return data_features\n",
    "\n",
    "def FeatureMerge(operation_features, transaction_features):\n",
    "    features = pd.merge(operation_features, transaction_features, on = \"UID\", how = \"outer\")\n",
    "    x = features.drop([\"UID\", \"Tag_x\", \"Tag_y\"], axis = 1)\n",
    "    y = features[\"Tag_x\"]\n",
    "    return x, y\n",
    "\n",
    "def FeatureFillNa(x):\n",
    "    x = x.fillna(-1)\n",
    "    return x\n",
    "\n",
    "def FeatureSelection(x_train, y_train, x_test):\n",
    "    sfm = SelectFromModel(GradientBoostingClassifier())\n",
    "    sfm.fit(x_train, y_train)\n",
    "    support = sfm.get_support()\n",
    "    indices = list(range(len(support)))\n",
    "    selected_indices = [index for index in indices if support[index]]\n",
    "    selected_features = x_train.columns.values[selected_indices]\n",
    "    x_train = x_train.loc[:, selected_features]\n",
    "    x_temp = pd.DataFrame(columns = selected_features)\n",
    "    for feature in selected_features:\n",
    "        if feature in x_test.columns:\n",
    "            x_temp[feature] = x_test[feature]\n",
    "    x_test = x_temp\n",
    "    return x_train, x_test\n",
    "\n",
    "def FeatureEngineering(operation_train, transaction_train, tag_train, operation_test, transaction_test, tag_test):\n",
    "    operation_train_features = FeatureCreation(operation_train, tag_train)\n",
    "    transaction_train_features = FeatureCreation(transaction_train, tag_train)\n",
    "    operation_test_features = FeatureCreation(operation_test, tag_test)\n",
    "    transaction_test_features = FeatureCreation(transaction_test, tag_test)\n",
    "\n",
    "    x_train, y_train = FeatureMerge(operation_train_features, transaction_train_features)\n",
    "    x_test, y_test = FeatureMerge(operation_test_features, transaction_test_features)\n",
    "\n",
    "    x_train = FeatureFillNa(x_train)\n",
    "    x_test = FeatureFillNa(x_test)\n",
    "\n",
    "#     x_train, x_test = FeatureSelection(x_train, y_train, x_test)\n",
    "    return x_train, y_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Step 4 : Model Optimization\n",
    "    \n",
    "    Models : lr, gbdt, xgb, lgbm\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def ModelOptimization(model, params, x_train, y_train):\n",
    "    best_params = []\n",
    "#     cv = GridSearchCV(estimator = model, param_grid = params, scoring = \"roc_auc\", cv = 3, n_jobs = -1)\n",
    "#     cv.fit(x_train, y_train)\n",
    "    for param in params:\n",
    "        print(\"Optimize param\", param, \"...\")\n",
    "        cv = GridSearchCV(estimator = model, param_grid = param, scoring = \"roc_auc\", cv = 3, n_jobs = -1)\n",
    "        cv.fit(x_train, y_train)\n",
    "        best_params.append(cv.best_params_)\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Step 5 : Model Evaluation\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def tpr_weight_function(y_true, y_predict):\n",
    "    d = pd.DataFrame()\n",
    "    d['prob'] = list(y_predict)\n",
    "    d['y'] = list(y_true)\n",
    "    d = d.sort_values(['prob'], ascending=[0])\n",
    "    y = d.y\n",
    "    PosAll = pd.Series(y).value_counts()[1]\n",
    "    NegAll = pd.Series(y).value_counts()[0]\n",
    "    pCumsum = d['y'].cumsum()\n",
    "    nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "    pCumsumPer = pCumsum / PosAll\n",
    "    nCumsumPer = nCumsum / NegAll\n",
    "    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "    return 0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3\n",
    "\n",
    "def ModelEvaluation(model, x_train, y_train):\n",
    "    # roc_auc\n",
    "    print(\"Compute roc_auc_score...\")\n",
    "    roc_auc = np.mean(cross_val_score(estimator = model, \n",
    "                                         X = x_train, \n",
    "                                         y = y_train, \n",
    "                                         scoring = \"roc_auc\", \n",
    "                                         cv = 3, \n",
    "                                         n_jobs = -1, \n",
    "                                         verbose = 10))\n",
    "\n",
    "    # tpr_weight\n",
    "    print(\"Compute tpr_weight_score...\")\n",
    "    kf = KFold(n_splits = 3)\n",
    "    model_scores = []\n",
    "    for train_index, test_index in kf.split(x_train):\n",
    "        print(\"Split data...\")\n",
    "        x_tr, x_te = x_train.values[train_index], x_train.values[test_index]\n",
    "        y_tr, y_te = y_train.values[train_index], y_train.values[test_index]\n",
    "        model.fit(x_tr, y_tr)\n",
    "        y_pred = model.predict(x_te)\n",
    "        score = tpr_weight_function(y_te, y_pred)\n",
    "        model_scores.append(score)\n",
    "    tpr_weight = np.mean(model_scores)\n",
    "    \n",
    "    return roc_auc, tpr_weight\n",
    "\n",
    "def Record(x_train, model, roc_auc_score, tpr_weight_score):\n",
    "    with open(RECORDS_PATH + \"record.txt\", \"a\") as f:\n",
    "        f.write(\"features:\\t\")\n",
    "        f.write(\"[\" + \", \".join(x_train.columns.values) + \"]\")\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"model:\\t\")\n",
    "        f.write(str(gbdt))\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"roc_auc_score:\\t\")\n",
    "        f.write(str(roc_auc_score))\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"tpr_weight_score:\\t\")\n",
    "        f.write(str(tpr_weight_score))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"#\" * 100)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Step 6 : Predict and Submit\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def PredictSubmit(model, x_train, y_train, x_test, tag_test, file_name):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict_proba(x_test)[:, 1]\n",
    "    submission = pd.DataFrame()\n",
    "    submission[\"UID\"] = tag_test[\"UID\"]\n",
    "    submission[\"Tag\"] = y_pred\n",
    "    submission.to_csv(SUBMISSION_PATH + file_name, index = False)\n",
    "    return y_pred, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Step 7 : Ensembling Predict Submit\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def EnsemblingPredictSubmit(y_pred_list, tag_test, file_name):\n",
    "    y_pred = np.array(y_pred_list).mean(axis = 0)\n",
    "    submission = pd.DataFrame()\n",
    "    submission[\"UID\"] = tag_test[\"UID\"]\n",
    "    submission[\"Tag\"] = y_pred\n",
    "    submission.to_csv(SUBMISSION_PATH + file_name, index = False)\n",
    "    return y_pred, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant Variables\n",
    "DATA_PATH = \"../Data/\"\n",
    "SUBMISSION_PATH = \"../Submission/\"\n",
    "RECORDS_PATH = \"../Records/\"\n",
    "RANDOM_STATE = 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3185: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    }
   ],
   "source": [
    "# Get Data\n",
    "operation_train, transaction_train, tag_train, operation_test, transaction_test = GetData(DATA_PATH)\n",
    "# operation_train, transaction_train, operation_test, transaction_test = operation_train[:10000], transaction_train[:10000], operation_test[:10000], transaction_test[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: \n",
      "data.shape:  (1460843, 21)\n",
      "after: \n",
      "data.shape:  (1234347, 21)\n",
      "before: \n",
      "data.shape:  (264654, 28)\n",
      "after: \n",
      "data.shape:  (264622, 28)\n",
      "before: \n",
      "data.shape:  (1769049, 21)\n",
      "after: \n",
      "data.shape:  (1578036, 21)\n",
      "before: \n",
      "data.shape:  (168981, 28)\n",
      "after: \n",
      "data.shape:  (168452, 28)\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "operation_train, transaction_train, tag_train, operation_test, transaction_test, tag_test = DataPreprocessing(operation_train, transaction_train, tag_train, operation_test, transaction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration\n",
    "# DataExploration(operation_train, transaction_train, train_mode = True)\n",
    "# DataExploration(operation_test, transaction_test, train_mode = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create day_nunique...\n",
      "Create day_30_count...\n",
      "Create day_16_count...\n",
      "Create day_8_count...\n",
      "Create day_23_count...\n",
      "Create day_26_count...\n",
      "Create day_27_count...\n",
      "Create day_19_count...\n",
      "Create day_5_count...\n",
      "Create day_13_count...\n",
      "Create day_10_count...\n",
      "Create day_1_count...\n",
      "Create day_3_count...\n",
      "Create day_20_count...\n",
      "Create day_18_count...\n",
      "Create day_22_count...\n",
      "Create day_28_count...\n",
      "Create day_12_count...\n",
      "Create day_4_count...\n",
      "Create day_15_count...\n",
      "Create day_9_count...\n",
      "Create day_6_count...\n",
      "Create day_2_count...\n",
      "Create day_25_count...\n",
      "Create day_21_count...\n",
      "Create day_11_count...\n",
      "Create day_17_count...\n",
      "Create day_29_count...\n",
      "Create day_24_count...\n",
      "Create day_7_count...\n",
      "Create day_14_count...\n",
      "Create mode_nunique...\n",
      "Create success_nunique...\n",
      "Create success_1.0_count...\n",
      "Create success_0.0_count...\n",
      "Create time_nunique...\n",
      "Create os_nunique...\n",
      "Create os_102_count...\n",
      "Create os_200_count...\n",
      "Create os_103_count...\n",
      "Create os_101_count...\n",
      "Create os_107_count...\n",
      "Create os_104_count...\n",
      "Create os_105_count...\n",
      "Create version_nunique...\n",
      "Create version_7.0.9_count...\n",
      "Create version_7.0.5_count...\n",
      "Create version_7.0.0_count...\n",
      "Create version_7.0.2_count...\n",
      "Create version_4.1.7_count...\n",
      "Create version_7.0.7_count...\n",
      "Create version_6.5.0_count...\n",
      "Create version_6.6.3_count...\n",
      "Create version_6.6.2_count...\n",
      "Create version_5.8.15_count...\n",
      "Create version_1.2.0_count...\n",
      "Create version_5.8.24_count...\n",
      "Create version_7.0.1_count...\n",
      "Create version_6.1.0_count...\n",
      "Create version_5.8.21_count...\n",
      "Create version_1.0.0_count...\n",
      "Create version_5.8.20_count...\n",
      "Create version_5.8.23_count...\n",
      "Create version_6.0.4_count...\n",
      "Create version_6.5.3_count...\n",
      "Create version_5.8.18_count...\n",
      "Create version_5.8.22_count...\n",
      "Create version_7.1.0_count...\n",
      "Create version_6.5.7_count...\n",
      "Create version_6.6.0_count...\n",
      "Create version_6.5.4_count...\n",
      "Create version_4.2.2_count...\n",
      "Create version_4.2.18.0522.r2_count...\n",
      "Create version_4.3.0_count...\n",
      "Create version_3.9.3_count...\n",
      "Create version_5.8.11_count...\n",
      "Create version_7.0.8_count...\n",
      "Create version_1.1.0_count...\n",
      "Create version_7.0.6_count...\n",
      "Create version_6.5.2_count...\n",
      "Create version_6.5.5_count...\n",
      "Create version_0.0.2_count...\n",
      "Create version_5.1.1_count...\n",
      "Create device1_nunique...\n",
      "Create device2_nunique...\n",
      "Create device_code1_nunique...\n",
      "Create device_code2_nunique...\n",
      "Create device_code3_nunique...\n",
      "Create mac1_nunique...\n",
      "Create mac2_nunique...\n",
      "Create ip1_nunique...\n",
      "Create ip2_nunique...\n",
      "Create wifi_nunique...\n",
      "Create geo_code_nunique...\n",
      "Create ip1_sub_nunique...\n",
      "Create ip2_sub_nunique...\n",
      "Create hour_nunique...\n",
      "Create hour_17_count...\n",
      "Create hour_8_count...\n",
      "Create hour_18_count...\n",
      "Create hour_23_count...\n",
      "Create hour_11_count...\n",
      "Create hour_16_count...\n",
      "Create hour_21_count...\n",
      "Create hour_9_count...\n",
      "Create hour_15_count...\n",
      "Create hour_14_count...\n",
      "Create hour_22_count...\n",
      "Create hour_10_count...\n",
      "Create hour_20_count...\n",
      "Create hour_12_count...\n",
      "Create hour_13_count...\n",
      "Create hour_7_count...\n",
      "Create hour_1_count...\n",
      "Create hour_19_count...\n",
      "Create hour_0_count...\n",
      "Create hour_3_count...\n",
      "Create hour_2_count...\n",
      "Create hour_6_count...\n",
      "Create hour_5_count...\n",
      "Create hour_4_count...\n",
      "Create minute_nunique...\n",
      "Create second_nunique...\n",
      "Create channel_nunique...\n",
      "Create channel_102_count...\n",
      "Create channel_140_count...\n",
      "Create channel_119_count...\n",
      "Create channel_106_count...\n",
      "Create channel_118_count...\n",
      "Create day_nunique...\n",
      "Create day_30_count...\n",
      "Create day_23_count...\n",
      "Create day_22_count...\n",
      "Create day_1_count...\n",
      "Create day_8_count...\n",
      "Create day_10_count...\n",
      "Create day_26_count...\n",
      "Create day_29_count...\n",
      "Create day_15_count...\n",
      "Create day_19_count...\n",
      "Create day_25_count...\n",
      "Create day_16_count...\n",
      "Create day_13_count...\n",
      "Create day_5_count...\n",
      "Create day_9_count...\n",
      "Create day_27_count...\n",
      "Create day_21_count...\n",
      "Create day_14_count...\n",
      "Create day_11_count...\n",
      "Create day_3_count...\n",
      "Create day_28_count...\n",
      "Create day_24_count...\n",
      "Create day_2_count...\n",
      "Create day_17_count...\n",
      "Create day_4_count...\n",
      "Create day_18_count...\n",
      "Create day_6_count...\n",
      "Create day_12_count...\n",
      "Create day_20_count...\n",
      "Create day_7_count...\n",
      "Create time_nunique...\n",
      "Create trans_amt_nunique...\n",
      "Create amt_src1_nunique...\n",
      "Create amt_src1_acdbdb842ac20f1e_count...\n",
      "Create amt_src1_4d7831c6f695ab19_count...\n",
      "Create amt_src1_c5fc631370cabc0d_count...\n",
      "Create amt_src1_a571c7fda8b7df37_count...\n",
      "Create amt_src1_d7de70fc65292e41_count...\n",
      "Create amt_src1_27c42480134c0d02_count...\n",
      "Create amt_src1_f29829bc82459191_count...\n",
      "Create amt_src1_155c9e1c32bd0fa2_count...\n",
      "Create amt_src1_9451ef3c5a0d6807_count...\n",
      "Create amt_src1_992d3ce08a4ca702_count...\n",
      "Create amt_src1_767001914a988cfb_count...\n",
      "Create amt_src1_b0a5496f0db7f70a_count...\n",
      "Create amt_src1_b3acdf321be07351_count...\n",
      "Create amt_src1_3045dd7701f3e263_count...\n",
      "Create amt_src1_8c9987909b3e95a4_count...\n",
      "Create amt_src1_8c753ae7afb60e61_count...\n",
      "Create amt_src1_79b1dd31895a6278_count...\n",
      "Create amt_src1_0b747ef141d49c8c_count...\n",
      "Create amt_src1_8576ce2e7ab65ac2_count...\n",
      "Create amt_src1_9a1fad6202fd1a24_count...\n",
      "Create amt_src1_4ec06e4d12f8560d_count...\n",
      "Create amt_src1_47b415a384665f45_count...\n",
      "Create amt_src1_d4f7a3699ef02458_count...\n",
      "Create amt_src1_d46a2a9577fb52c3_count...\n",
      "Create amt_src1_41c767468d03b4ac_count...\n",
      "Create amt_src1_295eadbc19cddb04_count...\n",
      "Create amt_src1_f2500fa92c7e39b9_count...\n",
      "Create amt_src1_fd4d2d1006a95637_count...\n",
      "Create merchant_nunique...\n",
      "Create code1_nunique...\n",
      "Create code2_nunique...\n",
      "Create trans_type1_nunique...\n",
      "Create trans_type1_26bcf43a19df14c8_count...\n",
      "Create trans_type1_c2f2023d279665b2_count...\n",
      "Create trans_type1_6d55c54c8b1056fb_count...\n",
      "Create trans_type1_61bfb66c928f36ac_count...\n",
      "Create trans_type1_e0d7b8768da99dd4_count...\n",
      "Create trans_type1_d9c417304a5ae70c_count...\n",
      "Create trans_type1_9d7dd7b80e806024_count...\n",
      "Create trans_type1_4adc3de71fe1a83c_count...\n",
      "Create trans_type1_a19e7a8951e54c06_count...\n",
      "Create trans_type1_ced62357ad496957_count...\n",
      "Create trans_type1_eb8d10591677bbe1_count...\n",
      "Create trans_type1_e903cf2a79b83d37_count...\n",
      "Create trans_type1_85bced5214d33ad2_count...\n",
      "Create trans_type1_fd4d2d1006a95637_count...\n",
      "Create trans_type1_3f469aa3836e71cb_count...\n",
      "Create acc_id1_nunique...\n",
      "Create device_code1_nunique...\n",
      "Create device_code2_nunique...\n",
      "Create device_code3_nunique...\n",
      "Create device1_nunique...\n",
      "Create device2_nunique...\n",
      "Create mac1_nunique...\n",
      "Create ip1_nunique...\n",
      "Create bal_nunique...\n",
      "Create amt_src2_nunique...\n",
      "Create acc_id2_nunique...\n",
      "Create acc_id3_nunique...\n",
      "Create geo_code_nunique...\n",
      "Create trans_type2_nunique...\n",
      "Create trans_type2_102.0_count...\n",
      "Create trans_type2_105.0_count...\n",
      "Create trans_type2_103.0_count...\n",
      "Create trans_type2_104.0_count...\n",
      "Create market_code_nunique...\n",
      "Create market_type_nunique...\n",
      "Create market_type_1.0_count...\n",
      "Create market_type_2.0_count...\n",
      "Create ip1_sub_nunique...\n",
      "Create hour_nunique...\n",
      "Create hour_11_count...\n",
      "Create hour_16_count...\n",
      "Create hour_9_count...\n",
      "Create hour_18_count...\n",
      "Create hour_13_count...\n",
      "Create hour_1_count...\n",
      "Create hour_10_count...\n",
      "Create hour_6_count...\n",
      "Create hour_14_count...\n",
      "Create hour_8_count...\n",
      "Create hour_19_count...\n",
      "Create hour_15_count...\n",
      "Create hour_21_count...\n",
      "Create hour_17_count...\n",
      "Create hour_12_count...\n",
      "Create hour_3_count...\n",
      "Create hour_7_count...\n",
      "Create hour_23_count...\n",
      "Create hour_20_count...\n",
      "Create hour_22_count...\n",
      "Create hour_0_count...\n",
      "Create hour_2_count...\n",
      "Create hour_5_count...\n",
      "Create hour_4_count...\n",
      "Create minute_nunique...\n",
      "Create second_nunique...\n",
      "Create day_nunique...\n",
      "Create day_8_count...\n",
      "Create day_20_count...\n",
      "Create day_19_count...\n",
      "Create day_16_count...\n",
      "Create day_6_count...\n",
      "Create day_23_count...\n",
      "Create day_25_count...\n",
      "Create day_31_count...\n",
      "Create day_9_count...\n",
      "Create day_14_count...\n",
      "Create day_30_count...\n",
      "Create day_10_count...\n",
      "Create day_11_count...\n",
      "Create day_18_count...\n",
      "Create day_27_count...\n",
      "Create day_12_count...\n",
      "Create day_1_count...\n",
      "Create day_7_count...\n",
      "Create day_3_count...\n",
      "Create day_21_count...\n",
      "Create day_5_count...\n",
      "Create day_2_count...\n",
      "Create day_4_count...\n",
      "Create day_15_count...\n",
      "Create day_28_count...\n",
      "Create day_13_count...\n",
      "Create day_17_count...\n",
      "Create day_22_count...\n",
      "Create day_24_count...\n",
      "Create day_26_count...\n",
      "Create day_29_count...\n",
      "Create mode_nunique...\n",
      "Create success_nunique...\n",
      "Create success_1.0_count...\n",
      "Create success_0.0_count...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create time_nunique...\n",
      "Create os_nunique...\n",
      "Create os_102_count...\n",
      "Create os_103_count...\n",
      "Create os_200_count...\n",
      "Create os_101_count...\n",
      "Create os_107_count...\n",
      "Create os_104_count...\n",
      "Create os_105_count...\n",
      "Create version_nunique...\n",
      "Create version_7.0.9_count...\n",
      "Create version_7.1.3_count...\n",
      "Create version_7.0.5_count...\n",
      "Create version_7.0.7_count...\n",
      "Create version_7.1.2_count...\n",
      "Create version_7.0.2_count...\n",
      "Create version_6.6.2_count...\n",
      "Create version_7.0.0_count...\n",
      "Create version_4.1.7_count...\n",
      "Create version_6.6.3_count...\n",
      "Create version_5.8.21_count...\n",
      "Create version_6.0.4_count...\n",
      "Create version_1.3.0_count...\n",
      "Create version_6.0.5_count...\n",
      "Create version_7.0.1_count...\n",
      "Create version_1.0.0_count...\n",
      "Create version_1.2.0_count...\n",
      "Create version_1.1.0_count...\n",
      "Create version_6.6.0_count...\n",
      "Create version_6.5.0_count...\n",
      "Create version_6.1.0_count...\n",
      "Create version_5.8.24_count...\n",
      "Create version_5.8.15_count...\n",
      "Create version_0.0.2_count...\n",
      "Create version_7.1.0_count...\n",
      "Create version_5.8.20_count...\n",
      "Create version_5.8.18_count...\n",
      "Create version_5.8.19_count...\n",
      "Create version_5.8.6_count...\n",
      "Create version_6.2.1_count...\n",
      "Create version_6.5.2_count...\n",
      "Create version_6.5.5_count...\n",
      "Create version_3.8.7_count...\n",
      "Create version_4.2.19.0702.r3_count...\n",
      "Create version_6.5.6_count...\n",
      "Create version_7.1.1_count...\n",
      "Create version_5.1.9_count...\n",
      "Create version_5.3.2_count...\n",
      "Create version_4.2.18.0522.r2_count...\n",
      "Create device1_nunique...\n",
      "Create device2_nunique...\n",
      "Create device_code1_nunique...\n",
      "Create device_code2_nunique...\n",
      "Create device_code3_nunique...\n",
      "Create mac1_nunique...\n",
      "Create mac2_nunique...\n",
      "Create ip1_nunique...\n",
      "Create ip2_nunique...\n",
      "Create wifi_nunique...\n",
      "Create geo_code_nunique...\n",
      "Create ip1_sub_nunique...\n",
      "Create ip2_sub_nunique...\n",
      "Create hour_nunique...\n",
      "Create hour_15_count...\n",
      "Create hour_16_count...\n",
      "Create hour_21_count...\n",
      "Create hour_6_count...\n",
      "Create hour_0_count...\n",
      "Create hour_13_count...\n",
      "Create hour_8_count...\n",
      "Create hour_10_count...\n",
      "Create hour_9_count...\n",
      "Create hour_20_count...\n",
      "Create hour_19_count...\n",
      "Create hour_12_count...\n",
      "Create hour_14_count...\n",
      "Create hour_3_count...\n",
      "Create hour_18_count...\n",
      "Create hour_11_count...\n",
      "Create hour_17_count...\n",
      "Create hour_22_count...\n",
      "Create hour_7_count...\n",
      "Create hour_23_count...\n",
      "Create hour_1_count...\n",
      "Create hour_4_count...\n",
      "Create hour_2_count...\n",
      "Create hour_5_count...\n",
      "Create minute_nunique...\n",
      "Create second_nunique...\n",
      "Create channel_nunique...\n",
      "Create channel_140_count...\n",
      "Create channel_102_count...\n",
      "Create channel_119_count...\n",
      "Create channel_106_count...\n",
      "Create channel_118_count...\n",
      "Create day_nunique...\n",
      "Create day_25_count...\n",
      "Create day_27_count...\n",
      "Create day_28_count...\n",
      "Create day_29_count...\n",
      "Create day_13_count...\n",
      "Create day_7_count...\n",
      "Create day_10_count...\n",
      "Create day_22_count...\n",
      "Create day_14_count...\n",
      "Create day_17_count...\n",
      "Create day_6_count...\n",
      "Create day_15_count...\n",
      "Create day_20_count...\n",
      "Create day_8_count...\n",
      "Create day_21_count...\n",
      "Create day_2_count...\n",
      "Create day_30_count...\n",
      "Create day_23_count...\n",
      "Create day_19_count...\n",
      "Create day_4_count...\n",
      "Create day_16_count...\n",
      "Create day_5_count...\n",
      "Create day_3_count...\n",
      "Create day_24_count...\n",
      "Create day_26_count...\n",
      "Create day_18_count...\n",
      "Create day_1_count...\n",
      "Create day_9_count...\n",
      "Create day_11_count...\n",
      "Create day_31_count...\n",
      "Create day_12_count...\n",
      "Create time_nunique...\n",
      "Create trans_amt_nunique...\n",
      "Create amt_src1_nunique...\n",
      "Create amt_src1_c5fc631370cabc0d_count...\n",
      "Create amt_src1_4d7831c6f695ab19_count...\n",
      "Create amt_src1_a571c7fda8b7df37_count...\n",
      "Create amt_src1_f29829bc82459191_count...\n",
      "Create amt_src1_155c9e1c32bd0fa2_count...\n",
      "Create amt_src1_27c42480134c0d02_count...\n",
      "Create amt_src1_9451ef3c5a0d6807_count...\n",
      "Create amt_src1_acdbdb842ac20f1e_count...\n",
      "Create amt_src1_992d3ce08a4ca702_count...\n",
      "Create amt_src1_8c753ae7afb60e61_count...\n",
      "Create amt_src1_8c9987909b3e95a4_count...\n",
      "Create amt_src1_9a1fad6202fd1a24_count...\n",
      "Create amt_src1_767001914a988cfb_count...\n",
      "Create amt_src1_b0a5496f0db7f70a_count...\n",
      "Create amt_src1_8576ce2e7ab65ac2_count...\n",
      "Create amt_src1_b3acdf321be07351_count...\n",
      "Create amt_src1_d7de70fc65292e41_count...\n",
      "Create amt_src1_4ec06e4d12f8560d_count...\n",
      "Create amt_src1_d46a2a9577fb52c3_count...\n",
      "Create amt_src1_79b1dd31895a6278_count...\n",
      "Create amt_src1_0b747ef141d49c8c_count...\n",
      "Create amt_src1_3045dd7701f3e263_count...\n",
      "Create amt_src1_d4f7a3699ef02458_count...\n",
      "Create amt_src1_41c767468d03b4ac_count...\n",
      "Create amt_src1_295eadbc19cddb04_count...\n",
      "Create amt_src1_47b415a384665f45_count...\n",
      "Create amt_src1_fd4d2d1006a95637_count...\n",
      "Create merchant_nunique...\n",
      "Create code1_nunique...\n",
      "Create code2_nunique...\n",
      "Create trans_type1_nunique...\n",
      "Create trans_type1_768160899ae359f6_count...\n",
      "Create trans_type1_c2f2023d279665b2_count...\n",
      "Create trans_type1_6d55c54c8b1056fb_count...\n",
      "Create trans_type1_61bfb66c928f36ac_count...\n",
      "Create trans_type1_26bcf43a19df14c8_count...\n",
      "Create trans_type1_d9c417304a5ae70c_count...\n",
      "Create trans_type1_85bced5214d33ad2_count...\n",
      "Create trans_type1_e0d7b8768da99dd4_count...\n",
      "Create trans_type1_9d7dd7b80e806024_count...\n",
      "Create trans_type1_ced62357ad496957_count...\n",
      "Create trans_type1_a19e7a8951e54c06_count...\n",
      "Create trans_type1_e903cf2a79b83d37_count...\n",
      "Create trans_type1_4adc3de71fe1a83c_count...\n",
      "Create trans_type1_eb8d10591677bbe1_count...\n",
      "Create trans_type1_fd4d2d1006a95637_count...\n",
      "Create trans_type1_3f469aa3836e71cb_count...\n",
      "Create acc_id1_nunique...\n",
      "Create device_code1_nunique...\n",
      "Create device_code2_nunique...\n",
      "Create device_code3_nunique...\n",
      "Create device1_nunique...\n",
      "Create device2_nunique...\n",
      "Create mac1_nunique...\n",
      "Create ip1_nunique...\n",
      "Create bal_nunique...\n",
      "Create amt_src2_nunique...\n",
      "Create acc_id2_nunique...\n",
      "Create acc_id3_nunique...\n",
      "Create geo_code_nunique...\n",
      "Create trans_type2_nunique...\n",
      "Create trans_type2_105.0_count...\n",
      "Create trans_type2_102.0_count...\n",
      "Create trans_type2_104.0_count...\n",
      "Create trans_type2_103.0_count...\n",
      "Create market_code_nunique...\n",
      "Create market_type_nunique...\n",
      "Create market_type_1.0_count...\n",
      "Create market_type_2.0_count...\n",
      "Create ip1_sub_nunique...\n",
      "Create hour_nunique...\n",
      "Create hour_11_count...\n",
      "Create hour_16_count...\n",
      "Create hour_6_count...\n",
      "Create hour_15_count...\n",
      "Create hour_8_count...\n",
      "Create hour_10_count...\n",
      "Create hour_22_count...\n",
      "Create hour_19_count...\n",
      "Create hour_9_count...\n",
      "Create hour_12_count...\n",
      "Create hour_14_count...\n",
      "Create hour_13_count...\n",
      "Create hour_20_count...\n",
      "Create hour_17_count...\n",
      "Create hour_1_count...\n",
      "Create hour_4_count...\n",
      "Create hour_7_count...\n",
      "Create hour_21_count...\n",
      "Create hour_23_count...\n",
      "Create hour_3_count...\n",
      "Create hour_18_count...\n",
      "Create hour_0_count...\n",
      "Create hour_5_count...\n",
      "Create hour_2_count...\n",
      "Create minute_nunique...\n",
      "Create second_nunique...\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering\n",
    "x_train, y_train, x_test = FeatureEngineering(operation_train, transaction_train, tag_train, operation_test, transaction_test, tag_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize param {'C': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10]} ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "g:\\python\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize param {'class_weight': [None, 'balanced']} ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "g:\\python\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize param {'max_iter': [100, 300, 500, 1000]} ...\n"
     ]
    }
   ],
   "source": [
    "# Model Optimization\n",
    "# params can be set as constant variable at the head of program\n",
    "lr = LogisticRegression(random_state = RANDOM_STATE)\n",
    "lr_params = [{\"C\": [0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10]}, \n",
    "             {\"class_weight\": [None, \"balanced\"]}, \n",
    "             {\"max_iter\": [100, 300, 500, 1000]}, \n",
    "             {\"penalty\": [\"l1\", \"l2\"]},\n",
    "             {\"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}]\n",
    "lr_best_params = ModelOptimization(lr, lr_params, x_train, y_train)\n",
    "\n",
    "gbdt = GradientBoostingClassifier(random_state = RANDOM_STATE)\n",
    "gbdt_params = [{\"n_estimators\": [100, 300, 500, 1000]}, \n",
    "               {\"learning_rate\": [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0]}, \n",
    "               {\"max_features\": [None, \"log2\", \"sqrt\"]}, \n",
    "               {\"max_depth\": [3, 5, 7, 9]}, \n",
    "               {\"min_samples_split\": [2, 4, 6, 8]}, \n",
    "               {\"min_samples_leaf\": [1, 3, 5, 7]}]\n",
    "gbdt_best_params = ModelOptimization(gbdt, gbdt_params, x_train, y_train)\n",
    "                \n",
    "xgb = XGBClassifier(random_state = RANDOM_STATE)\n",
    "xgb_params = [{\"learning_rate\": [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1]}, \n",
    "              {\"n_estimators\": [100, 300, 500, 1000]}, \n",
    "              {\"max_depth\": range(3,10,2)}, \n",
    "              {\"min_child_weight\": range(1,6,2)}, \n",
    "              {\"gamma\": [i/10.0 for i in range(0,5)]}, \n",
    "              {\"subsample\": [i/10.0 for i in range(6,10)]},\n",
    "              {\"colsample_bytree\": [i/10.0 for i in range(6,10)]}, \n",
    "              {\"reg_alpha\": [1e-5, 1e-2, 0.1, 1, 100]}]\n",
    "xgb_best_params = ModelOptimization(xgb, xgb_params, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "lr = LogisticRegression(C = 3.0, class_weight = \"balanced\", max_iter = 100, penalty = \"l2\", solver = \"newton-cg\")\n",
    "lr_roc_auc_score, lr_tpr_weight_score = ModelEvaluation(lr, x_train, y_train)\n",
    "Record(x_train, lr, lr_roc_auc_score, lr_tpr_weight_score)\n",
    "\n",
    "gbdt = GradientBoostingClassifier(n_estimators = 1000, learning_rate = 0.3, max_features = None, \n",
    "                                  max_depth = 5, min_samples_split = 6, min_samples_leaf = 1)\n",
    "gbdt_roc_auc_score, gbdt_tpr_weight_score = ModelEvaluation(gbdt, x_train, y_train)\n",
    "Record(x_train, gbdt, gbdt_roc_auc_score, gbdt_tpr_weight_score)\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = 500, learning_rate = 0.3, max_depth = 9, min_child_weight = 5, \n",
    "                    gamma = 0.3, subsample = 0.7, colsample_bytree = 0.7, reg_alpha = 1e-05)\n",
    "xgb_roc_auc_score, xgb_tpr_weight_score = ModelEvaluation(xgb, x_train, y_train)\n",
    "Record(x_train, xgb, xgb_roc_auc_score, xgb_tpr_weight_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Ensemble \n",
    "# Record the feature, model, params, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and Submit\n",
    "# lr_pred, lr_submission = PredictSubmit(lr, x_train, y_train, x_test, tag_test, \"lr.csv\")\n",
    "# gbdt_pred, gbdt_submission = PredictSubmit(gbdt, x_train, y_train, x_test, tag_test, \"gbdt.csv\")\n",
    "# xgb_pred, xgb_submission = PredictSubmit(xgb, x_train, y_train, x_test, tag_test, \"xgb.csv\")\n",
    "\n",
    "# No Feature Selection\n",
    "lr_pred, lr_submission = PredictSubmit(lr, x_train, y_train, x_test, tag_test, \"lr_no_feature_selection.csv\")\n",
    "gbdt_pred, gbdt_submission = PredictSubmit(gbdt, x_train, y_train, x_test, tag_test, \"gbdt_no_feature_selection.csv\")\n",
    "xgb_pred, xgb_submission = PredictSubmit(xgb, x_train, y_train, x_test, tag_test, \"xgb_no_feature_selection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembling Predict and Submit\n",
    "# ensemble_pred, ensemble_submission = EnsemblingPredictSubmit([lr_pred, gbdt_pred, xgb_pred], tag_test, \"ensemble.csv\")\n",
    "\n",
    "# No Feature Selection\n",
    "ensemble_pred, ensemble_submission = EnsemblingPredictSubmit([lr_pred, gbdt_pred, xgb_pred], tag_test, \"ensemble_no_feature_selection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
