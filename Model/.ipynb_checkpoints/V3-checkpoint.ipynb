{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time \n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Step 0 : Get Data\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def GetData(data_path):\n",
    "    print(\"Get Data Start\")\n",
    "    operation_train = pd.read_csv(data_path + \"operation_train_new.csv\")\n",
    "    transaction_train = pd.read_csv(data_path + \"transaction_train_new.csv\")\n",
    "    operation_test = pd.read_csv(data_path + \"operation_round1_new.csv\")\n",
    "    transaction_test = pd.read_csv(data_path + \"transaction_round1_new.csv\")\n",
    "    tag_train = pd.read_csv(data_path + \"tag_train_new.csv\")\n",
    "    tag_test = pd.read_csv(\"../Data/submission_example.csv\")\n",
    "    print(\"Get Data Done\")\n",
    "    return operation_train, transaction_train, tag_train, operation_test, transaction_test, tag_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Step 1 : Data Preprocessing \n",
    "\n",
    "    Drop Duplicates\n",
    "    Drop Outliers: After Data Exploration \n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def DataPreprocessing(data):\n",
    "    print(\"Data Preprocessing Start\")\n",
    "    data = data.drop_duplicates()\n",
    "    print(\"Data Preprocessing Done\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Step 2 : Data Exploration\n",
    "    \n",
    "    Categorical Columns : countplot, barplot\n",
    "    Numerical Columns : regplot\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def DataExploration(data, tag = None):\n",
    "    print(\"Data Exploration Start\")\n",
    "    print(\"Countplot\")\n",
    "    plt.figure(figsize = (15, 10))\n",
    "    i = 1\n",
    "    for column in data.columns:\n",
    "        if column != \"UID\" and data[column].nunique() < 50:\n",
    "            plt.subplot(2, 4, i)\n",
    "            sns.countplot(x = column, data = data)\n",
    "            i += 1\n",
    "    plt.suptitle(\"Countplot\")\n",
    "    plt.show()\n",
    "\n",
    "    if tag is not None:\n",
    "        uid_tag_dict = dict(zip(tag[\"UID\"], tag[\"Tag\"]))\n",
    "        data[\"tag\"] = data[\"UID\"].map(dict)\n",
    "        print(\"#\" * 100)\n",
    "        print(\"Barplot\")\n",
    "        plt.figure(figsize = (15, 10))\n",
    "        i = 1\n",
    "        for column in data.columns:\n",
    "            if data[column].unique().shape[0] < 50 and column != \"tag\":\n",
    "                print(\"Plot \" + column + \"...\")\n",
    "                plt.subplot(2, 4, i)\n",
    "                sns.barplot(x = column, y = \"tag\", data = data.sample(100000))\n",
    "                i += 1\n",
    "        plt.suptitle(\"Barplot\")\n",
    "        plt.show()\n",
    "        \n",
    "    print(\"Data Exploration Done\")\n",
    "    \n",
    "# operation\n",
    "# 时间日期：day, time\n",
    "# 操作：mode, success\n",
    "# 操作系统：os\n",
    "# 版本：version\n",
    "# 设备：device1, device2, device_code1, device_code2, device_code3\n",
    "# ip：ip1, ip2, ip1_sub, ip2_sub\n",
    "# mac：mac1, mac2\n",
    "# wifi：wifi\n",
    "# 地理位置：geo_code\n",
    "\n",
    "# transaction\n",
    "# 平台：channel\n",
    "# 日期时间：day, time\n",
    "# 资金：trans_amt, bal, amt_src1, amt_src2\n",
    "# 商户：merchant, code1, code2\n",
    "# 交易类型：trans_type1, trans_typ2\n",
    "# 账户：acc_id1, acc_id2\n",
    "# 设备：device_code1, device_code2, device_code3, device1, device2\n",
    "# ip：ip1, ip1_sub\n",
    "# mac：mac1\n",
    "# 地理位置：geocode\n",
    "# 营销活动：market_code, market_type\n",
    "\n",
    "# operation countplot结论\n",
    "# 1 日期分布较为均匀，操作记录较多的日期有1，8，15，22，29\n",
    "# 2 绝大多数操作记录都成功了\n",
    "# 3 操作系统分布不均，最多的是102，最少的是101和107\n",
    "# 4 版本中，操作记录最多的是7.0.9和7.0.5\n",
    "# 5 黑白样本记录不平衡，黑记录 / 白记录 = 137371 / 1096976 = 0.125；黑白样本比例不平衡，黑样本 / 白样本 = 4285 / 26894 = 0.16\n",
    "\n",
    "# operation barplot结论\n",
    "# 1 日期分布较为均匀，27附近的黑样本比例较高\n",
    "# 2 操作成功的黑样本比例较高\n",
    "# 3 操作系统为107的样本均为白样本，105的样本均为黑样本\n",
    "# 4 版本为6.1.0，4.1.7的样本均为黑样本，6.5.0，7.0.0，6.6.3的黑样本比例较高，许多版本的样本均为白样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Step 3 : Feature Engineering\n",
    "    \n",
    "    Feature Creation\n",
    "    Feature Selection\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def FeatureCreation(data, tag):\n",
    "    print(\"Feature Creation Start\")\n",
    "    data[\"hour\"] = data[\"time\"].apply(lambda x : int(x[:2]))\n",
    "    features = pd.DataFrame(tag[\"UID\"])\n",
    "    \n",
    "    # feature1: column_nunique\n",
    "    for column in data.columns:\n",
    "        if column != \"UID\":\n",
    "            print(\"Create \" + column + \"_nunique...\")\n",
    "            column_nunique = data.groupby(\"UID\")[column].agg([\"nunique\"]).reset_index().rename(columns = {\"nunique\": column + \"_nunique\"})\n",
    "            features = features.merge(column_nunique, on = \"UID\", how = \"left\")\n",
    "\n",
    "    # feature2: column_nunique_UID and column_count_UID\n",
    "    data_copy = data.copy()\n",
    "    for column in data.columns: \n",
    "        if column != \"UID\":\n",
    "            print(\"Create \" + column + \"_nunique_UID and \" + column + \"_count_UID...\")\n",
    "            column_nunique_count = data_copy.groupby(column)[\"UID\"].agg([\"nunique\", \"count\"]).reset_index().rename(columns = {\"nunique\": column + \"_nunique_UID\", \"count\": column + \"_count_UID\"})\n",
    "            data_copy = data_copy.merge(column_nunique_count, on = column, how = \"left\")\n",
    "\n",
    "    column_nunique = [col + \"_nunique_UID\" for col in data.columns if col != \"UID\"]\n",
    "    column_count = [col + \"_count_UID\" for col in data.columns if col != \"UID\"]\n",
    "    columns = column_nunique + column_count\n",
    "    \n",
    "    for column in columns:\n",
    "        print(\"Create \" + column + \"...\")\n",
    "        column_nunique_count_UID = data_copy.groupby(\"UID\")[column].agg([\"max\", \"min\", \"mean\"]).reset_index().rename(columns = {\"max\": column + \"_max\", \"min\": column + \"_min\", \"mean\": column + \"_mean\"})\n",
    "        features = features.merge(column_nunique_count_UID, on = \"UID\", how = \"left\")\n",
    "    \n",
    "    # feature3: day_frequency, hour_frequency\n",
    "    print(\"Create day_frequency...\")  \n",
    "    day_frequency = data.groupby([\"UID\", \"day\"])[\"time\"].agg([\"count\"]).reset_index().groupby(\"UID\")[\"count\"].agg([\"max\", \"min\", \"mean\"]).rename(columns = {\"max\": \"day_frequency_max\", \"min\": \"day_frequency_min\", \"mean\": \"day_frequency_mean\"})\n",
    "    features = features.merge(day_frequency, on = \"UID\", how = \"left\")\n",
    "    hour_frequency = data.groupby([\"UID\", \"day\", \"hour\"])[\"time\"].agg([\"count\"]).reset_index().groupby(\"UID\")[\"count\"].agg([\"max\", \"min\", \"mean\"]).rename(columns = {\"max\": \"hour_frequency_max\", \"min\": \"hour_frequency_min\", \"mean\": \"hour_frequency_mean\"})\n",
    "    features = features.merge(hour_frequency, on = \"UID\", how = \"left\")\n",
    "\n",
    "    print(\"Feature Creation Done\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "def FeatureSelection(x_train, y_train, x_test):\n",
    "    print(\"Feature Selection Start\")\n",
    "    sfm = SelectFromModel(GradientBoostingClassifier())\n",
    "    sfm.fit(x_train, y_train)\n",
    "    support = sfm.get_support()\n",
    "    indices = list(range(len(support)))\n",
    "    selected_indices = [index for index in indices if support[index]]\n",
    "    selected_features = x_train.columns.values[selected_indices]\n",
    "    x_train = x_train.loc[:, selected_features]\n",
    "    x_temp = pd.DataFrame(columns = selected_features)\n",
    "    for feature in selected_features:\n",
    "        if feature in x_test.columns:\n",
    "            x_temp[feature] = x_test[feature]\n",
    "    x_test = x_temp\n",
    "    print(\"Feature Selection Done\")\n",
    "    return x_train, x_test\n",
    "\n",
    "def FeatureEngineering(operation_train, transaction_train, tag_train, operation_test, transaction_test, tag_test):\n",
    "    print(\"Feature Engineering Start\")\n",
    "    operation_train_features = FeatureCreation(operation_train, tag_train)\n",
    "    transaction_train_features = FeatureCreation(transaction_train, tag_train)\n",
    "    operation_test_features = FeatureCreation(operation_test, tag_test)\n",
    "    transaction_test_features = FeatureCreation(transaction_test, tag_test)\n",
    "    x_train = operation_train_features.merge(transaction_train_features, on = \"UID\", how = \"left\")\n",
    "    y_train = tag_train[\"Tag\"]\n",
    "    x_test = operation_test_features.merge(transaction_test_features, on = \"UID\", how = \"left\")\n",
    "    x_train = x_train.fillna(-1)\n",
    "    x_test = x_test.fillna(-1)\n",
    "    x_train, x_test = FeatureSelection(x_train, y_train, x_test)\n",
    "    print(\"Feature Engineering Done\")\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Step 4 : Model Optimization\n",
    "    \n",
    "    Models : lr, gbdt, xgb\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def ModelOptimization(model, params, x_train, y_train):\n",
    "    print(\"Model Optimizatioin Start\")\n",
    "    x_train = x_train.fillna(-1)\n",
    "    best_params = []\n",
    "#     cv = GridSearchCV(estimator = model, param_grid = params, scoring = \"roc_auc\", cv = 3, n_jobs = -1)\n",
    "#     cv.fit(x_train, y_train)\n",
    "    for param in params:\n",
    "        print(\"Optimize param\", param, \"...\")\n",
    "        cv = GridSearchCV(estimator = model, param_grid = param, scoring = \"roc_auc\", cv = 3, n_jobs = -1)\n",
    "        cv.fit(x_train, y_train)\n",
    "        best_params.append(cv.best_params_)\n",
    "    print(\"Model Optimizatioin Done\")\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Step 5 : Model Evaluation\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def tpr_weight_function(y_true, y_predict):\n",
    "    d = pd.DataFrame()\n",
    "    d['prob'] = list(y_predict)\n",
    "    d['y'] = list(y_true)\n",
    "    d = d.sort_values(['prob'], ascending=[0])\n",
    "    y = d.y\n",
    "    PosAll = pd.Series(y).value_counts()[1]\n",
    "    NegAll = pd.Series(y).value_counts()[0]\n",
    "    pCumsum = d['y'].cumsum()\n",
    "    nCumsum = np.arange(len(y)) - pCumsum + 1\n",
    "    pCumsumPer = pCumsum / PosAll\n",
    "    nCumsumPer = nCumsum / NegAll\n",
    "    TR1 = pCumsumPer[abs(nCumsumPer-0.001).idxmin()]\n",
    "    TR2 = pCumsumPer[abs(nCumsumPer-0.005).idxmin()]\n",
    "    TR3 = pCumsumPer[abs(nCumsumPer-0.01).idxmin()]\n",
    "    return 0.4 * TR1 + 0.3 * TR2 + 0.3 * TR3\n",
    "\n",
    "def ModelEvaluation(model, x_train, y_train):\n",
    "    print(\"Model Evaluation Start\")\n",
    "    x_train = x_train.fillna(-1)\n",
    "    # roc_auc\n",
    "    print(\"Compute roc_auc_score...\")\n",
    "    roc_auc = np.mean(cross_val_score(estimator = model, \n",
    "                                         X = x_train, \n",
    "                                         y = y_train, \n",
    "                                         scoring = \"roc_auc\", \n",
    "                                         cv = 3, \n",
    "                                         n_jobs = -1, \n",
    "                                         verbose = 10))\n",
    "\n",
    "    # tpr_weight\n",
    "    print(\"Compute tpr_weight_score...\")\n",
    "    kf = KFold(n_splits = 3)\n",
    "    model_scores = []\n",
    "    for train_index, test_index in kf.split(x_train):\n",
    "        print(\"Split data...\")\n",
    "        x_tr, x_te = x_train.values[train_index], x_train.values[test_index]\n",
    "        y_tr, y_te = y_train.values[train_index], y_train.values[test_index]\n",
    "        model.fit(x_tr, y_tr)\n",
    "        y_pred = model.predict(x_te)\n",
    "        score = tpr_weight_function(y_te, y_pred)\n",
    "        model_scores.append(score)\n",
    "    tpr_weight = np.mean(model_scores)\n",
    "    print(\"Model Evaluation Done\")\n",
    "    return roc_auc, tpr_weight\n",
    "\n",
    "def Record(x_train, model, roc_auc_score, tpr_weight_score):\n",
    "    print(\"Record Start\")\n",
    "    with open(\"../Records/record.txt\", \"a\") as f:\n",
    "        f.write(\"features:\\t\")\n",
    "        f.write(\"[\" + \", \".join(x_train.columns.values) + \"]\")\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"model:\\t\")\n",
    "        f.write(str(model))\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"roc_auc_score:\\t\")\n",
    "        f.write(str(roc_auc_score))\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"tpr_weight_score:\\t\")\n",
    "        f.write(str(tpr_weight_score))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"#\" * 100)\n",
    "        f.write(\"\\n\")\n",
    "    print(\"Record Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Step 6 : Fit and Predict\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "def FitPredict(model, x_train, y_train, x_test):\n",
    "    print(\"Fit Predict Start\")\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict_proba(x_test)[:, 1]\n",
    "    print(\"Fit Predict Done\")\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Step 7 : Ensembling\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def Ensembling(y_pred_list, tag_test):\n",
    "    print(\"Ensembling Start\")\n",
    "    ensembling_y_pred = np.array(y_pred_list).mean(axis = 0)\n",
    "    print(\"Ensembling Done\")\n",
    "    return ensembling_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Step 8 : Submit\n",
    "    \n",
    "\"\"\"\n",
    "def Submit(y_pred, tag_test):\n",
    "    print(\"Submit Start\")\n",
    "    submission = pd.DataFrame({\"UID\": tag_test[\"UID\"], \"Tag\": y_pred})\n",
    "    print(\"Submit Done\")\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Data Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3185: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Data Done\n",
      "Data Preprocessing Start\n",
      "Data Preprocessing Done\n",
      "Data Preprocessing Start\n",
      "Data Preprocessing Done\n",
      "Data Preprocessing Start\n",
      "Data Preprocessing Done\n",
      "Data Preprocessing Start\n",
      "Data Preprocessing Done\n",
      "Feature Engineering Start\n",
      "Feature Creation Start\n",
      "Create day_nunique...\n",
      "Create mode_nunique...\n",
      "Create success_nunique...\n",
      "Create time_nunique...\n",
      "Create os_nunique...\n",
      "Create version_nunique...\n",
      "Create device1_nunique...\n",
      "Create device2_nunique...\n",
      "Create device_code1_nunique...\n",
      "Create device_code2_nunique...\n",
      "Create device_code3_nunique...\n",
      "Create mac1_nunique...\n",
      "Create mac2_nunique...\n",
      "Create ip1_nunique...\n",
      "Create ip2_nunique...\n",
      "Create wifi_nunique...\n",
      "Create geo_code_nunique...\n",
      "Create ip1_sub_nunique...\n",
      "Create ip2_sub_nunique...\n",
      "Create hour_nunique...\n",
      "Create day_nunique_UID and day_count_UID...\n",
      "Create mode_nunique_UID and mode_count_UID...\n",
      "Create success_nunique_UID and success_count_UID...\n",
      "Create time_nunique_UID and time_count_UID...\n",
      "Create os_nunique_UID and os_count_UID...\n",
      "Create version_nunique_UID and version_count_UID...\n",
      "Create device1_nunique_UID and device1_count_UID...\n",
      "Create device2_nunique_UID and device2_count_UID...\n",
      "Create device_code1_nunique_UID and device_code1_count_UID...\n",
      "Create device_code2_nunique_UID and device_code2_count_UID...\n",
      "Create device_code3_nunique_UID and device_code3_count_UID...\n",
      "Create mac1_nunique_UID and mac1_count_UID...\n",
      "Create mac2_nunique_UID and mac2_count_UID...\n",
      "Create ip1_nunique_UID and ip1_count_UID...\n",
      "Create ip2_nunique_UID and ip2_count_UID...\n",
      "Create wifi_nunique_UID and wifi_count_UID...\n",
      "Create geo_code_nunique_UID and geo_code_count_UID...\n",
      "Create ip1_sub_nunique_UID and ip1_sub_count_UID...\n",
      "Create ip2_sub_nunique_UID and ip2_sub_count_UID...\n",
      "Create hour_nunique_UID and hour_count_UID...\n",
      "Create day_nunique_UID...\n",
      "Create mode_nunique_UID...\n",
      "Create success_nunique_UID...\n",
      "Create time_nunique_UID...\n",
      "Create os_nunique_UID...\n",
      "Create version_nunique_UID...\n",
      "Create device1_nunique_UID...\n",
      "Create device2_nunique_UID...\n",
      "Create device_code1_nunique_UID...\n",
      "Create device_code2_nunique_UID...\n",
      "Create device_code3_nunique_UID...\n",
      "Create mac1_nunique_UID...\n",
      "Create mac2_nunique_UID...\n",
      "Create ip1_nunique_UID...\n",
      "Create ip2_nunique_UID...\n",
      "Create wifi_nunique_UID...\n",
      "Create geo_code_nunique_UID...\n",
      "Create ip1_sub_nunique_UID...\n",
      "Create ip2_sub_nunique_UID...\n",
      "Create hour_nunique_UID...\n",
      "Create day_count_UID...\n",
      "Create mode_count_UID...\n",
      "Create success_count_UID...\n",
      "Create time_count_UID...\n",
      "Create os_count_UID...\n",
      "Create version_count_UID...\n",
      "Create device1_count_UID...\n",
      "Create device2_count_UID...\n",
      "Create device_code1_count_UID...\n",
      "Create device_code2_count_UID...\n",
      "Create device_code3_count_UID...\n",
      "Create mac1_count_UID...\n",
      "Create mac2_count_UID...\n",
      "Create ip1_count_UID...\n",
      "Create ip2_count_UID...\n",
      "Create wifi_count_UID...\n",
      "Create geo_code_count_UID...\n",
      "Create ip1_sub_count_UID...\n",
      "Create ip2_sub_count_UID...\n",
      "Create hour_count_UID...\n",
      "Create day_frequency...\n",
      "Feature Creation Done\n",
      "Feature Creation Start\n",
      "Create channel_nunique...\n",
      "Create day_nunique...\n",
      "Create time_nunique...\n",
      "Create trans_amt_nunique...\n",
      "Create amt_src1_nunique...\n",
      "Create merchant_nunique...\n",
      "Create code1_nunique...\n",
      "Create code2_nunique...\n",
      "Create trans_type1_nunique...\n",
      "Create acc_id1_nunique...\n",
      "Create device_code1_nunique...\n",
      "Create device_code2_nunique...\n",
      "Create device_code3_nunique...\n",
      "Create device1_nunique...\n",
      "Create device2_nunique...\n",
      "Create mac1_nunique...\n",
      "Create ip1_nunique...\n",
      "Create bal_nunique...\n",
      "Create amt_src2_nunique...\n",
      "Create acc_id2_nunique...\n",
      "Create acc_id3_nunique...\n",
      "Create geo_code_nunique...\n",
      "Create trans_type2_nunique...\n",
      "Create market_code_nunique...\n",
      "Create market_type_nunique...\n",
      "Create ip1_sub_nunique...\n",
      "Create hour_nunique...\n",
      "Create channel_nunique_UID and channel_count_UID...\n",
      "Create day_nunique_UID and day_count_UID...\n",
      "Create time_nunique_UID and time_count_UID...\n",
      "Create trans_amt_nunique_UID and trans_amt_count_UID...\n",
      "Create amt_src1_nunique_UID and amt_src1_count_UID...\n",
      "Create merchant_nunique_UID and merchant_count_UID...\n",
      "Create code1_nunique_UID and code1_count_UID...\n",
      "Create code2_nunique_UID and code2_count_UID...\n",
      "Create trans_type1_nunique_UID and trans_type1_count_UID...\n",
      "Create acc_id1_nunique_UID and acc_id1_count_UID...\n",
      "Create device_code1_nunique_UID and device_code1_count_UID...\n",
      "Create device_code2_nunique_UID and device_code2_count_UID...\n",
      "Create device_code3_nunique_UID and device_code3_count_UID...\n",
      "Create device1_nunique_UID and device1_count_UID...\n",
      "Create device2_nunique_UID and device2_count_UID...\n",
      "Create mac1_nunique_UID and mac1_count_UID...\n",
      "Create ip1_nunique_UID and ip1_count_UID...\n",
      "Create bal_nunique_UID and bal_count_UID...\n",
      "Create amt_src2_nunique_UID and amt_src2_count_UID...\n",
      "Create acc_id2_nunique_UID and acc_id2_count_UID...\n",
      "Create acc_id3_nunique_UID and acc_id3_count_UID...\n",
      "Create geo_code_nunique_UID and geo_code_count_UID...\n",
      "Create trans_type2_nunique_UID and trans_type2_count_UID...\n",
      "Create market_code_nunique_UID and market_code_count_UID...\n",
      "Create market_type_nunique_UID and market_type_count_UID...\n",
      "Create ip1_sub_nunique_UID and ip1_sub_count_UID...\n",
      "Create hour_nunique_UID and hour_count_UID...\n",
      "Create channel_nunique_UID...\n",
      "Create day_nunique_UID...\n",
      "Create time_nunique_UID...\n",
      "Create trans_amt_nunique_UID...\n",
      "Create amt_src1_nunique_UID...\n",
      "Create merchant_nunique_UID...\n",
      "Create code1_nunique_UID...\n",
      "Create code2_nunique_UID...\n",
      "Create trans_type1_nunique_UID...\n",
      "Create acc_id1_nunique_UID...\n",
      "Create device_code1_nunique_UID...\n",
      "Create device_code2_nunique_UID...\n",
      "Create device_code3_nunique_UID...\n",
      "Create device1_nunique_UID...\n",
      "Create device2_nunique_UID...\n",
      "Create mac1_nunique_UID...\n",
      "Create ip1_nunique_UID...\n",
      "Create bal_nunique_UID...\n",
      "Create amt_src2_nunique_UID...\n",
      "Create acc_id2_nunique_UID...\n",
      "Create acc_id3_nunique_UID...\n",
      "Create geo_code_nunique_UID...\n",
      "Create trans_type2_nunique_UID...\n",
      "Create market_code_nunique_UID...\n",
      "Create market_type_nunique_UID...\n",
      "Create ip1_sub_nunique_UID...\n",
      "Create hour_nunique_UID...\n",
      "Create channel_count_UID...\n",
      "Create day_count_UID...\n",
      "Create time_count_UID...\n",
      "Create trans_amt_count_UID...\n",
      "Create amt_src1_count_UID...\n",
      "Create merchant_count_UID...\n",
      "Create code1_count_UID...\n",
      "Create code2_count_UID...\n",
      "Create trans_type1_count_UID...\n",
      "Create acc_id1_count_UID...\n",
      "Create device_code1_count_UID...\n",
      "Create device_code2_count_UID...\n",
      "Create device_code3_count_UID...\n",
      "Create device1_count_UID...\n",
      "Create device2_count_UID...\n",
      "Create mac1_count_UID...\n",
      "Create ip1_count_UID...\n",
      "Create bal_count_UID...\n",
      "Create amt_src2_count_UID...\n",
      "Create acc_id2_count_UID...\n",
      "Create acc_id3_count_UID...\n",
      "Create geo_code_count_UID...\n",
      "Create trans_type2_count_UID...\n",
      "Create market_code_count_UID...\n",
      "Create market_type_count_UID...\n",
      "Create ip1_sub_count_UID...\n",
      "Create hour_count_UID...\n",
      "Create day_frequency...\n",
      "Feature Creation Done\n",
      "Feature Creation Start\n",
      "Create day_nunique...\n",
      "Create mode_nunique...\n",
      "Create success_nunique...\n",
      "Create time_nunique...\n",
      "Create os_nunique...\n",
      "Create version_nunique...\n",
      "Create device1_nunique...\n",
      "Create device2_nunique...\n",
      "Create device_code1_nunique...\n",
      "Create device_code2_nunique...\n",
      "Create device_code3_nunique...\n",
      "Create mac1_nunique...\n",
      "Create mac2_nunique...\n",
      "Create ip1_nunique...\n",
      "Create ip2_nunique...\n",
      "Create wifi_nunique...\n",
      "Create geo_code_nunique...\n",
      "Create ip1_sub_nunique...\n",
      "Create ip2_sub_nunique...\n",
      "Create hour_nunique...\n",
      "Create day_nunique_UID and day_count_UID...\n",
      "Create mode_nunique_UID and mode_count_UID...\n",
      "Create success_nunique_UID and success_count_UID...\n",
      "Create time_nunique_UID and time_count_UID...\n",
      "Create os_nunique_UID and os_count_UID...\n",
      "Create version_nunique_UID and version_count_UID...\n",
      "Create device1_nunique_UID and device1_count_UID...\n",
      "Create device2_nunique_UID and device2_count_UID...\n",
      "Create device_code1_nunique_UID and device_code1_count_UID...\n",
      "Create device_code2_nunique_UID and device_code2_count_UID...\n",
      "Create device_code3_nunique_UID and device_code3_count_UID...\n",
      "Create mac1_nunique_UID and mac1_count_UID...\n",
      "Create mac2_nunique_UID and mac2_count_UID...\n",
      "Create ip1_nunique_UID and ip1_count_UID...\n",
      "Create ip2_nunique_UID and ip2_count_UID...\n",
      "Create wifi_nunique_UID and wifi_count_UID...\n",
      "Create geo_code_nunique_UID and geo_code_count_UID...\n",
      "Create ip1_sub_nunique_UID and ip1_sub_count_UID...\n",
      "Create ip2_sub_nunique_UID and ip2_sub_count_UID...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create hour_nunique_UID and hour_count_UID...\n",
      "Create day_nunique_UID...\n",
      "Create mode_nunique_UID...\n",
      "Create success_nunique_UID...\n",
      "Create time_nunique_UID...\n",
      "Create os_nunique_UID...\n",
      "Create version_nunique_UID...\n",
      "Create device1_nunique_UID...\n",
      "Create device2_nunique_UID...\n",
      "Create device_code1_nunique_UID...\n",
      "Create device_code2_nunique_UID...\n",
      "Create device_code3_nunique_UID...\n",
      "Create mac1_nunique_UID...\n",
      "Create mac2_nunique_UID...\n",
      "Create ip1_nunique_UID...\n",
      "Create ip2_nunique_UID...\n",
      "Create wifi_nunique_UID...\n",
      "Create geo_code_nunique_UID...\n",
      "Create ip1_sub_nunique_UID...\n",
      "Create ip2_sub_nunique_UID...\n",
      "Create hour_nunique_UID...\n",
      "Create day_count_UID...\n",
      "Create mode_count_UID...\n",
      "Create success_count_UID...\n",
      "Create time_count_UID...\n",
      "Create os_count_UID...\n",
      "Create version_count_UID...\n",
      "Create device1_count_UID...\n",
      "Create device2_count_UID...\n",
      "Create device_code1_count_UID...\n",
      "Create device_code2_count_UID...\n",
      "Create device_code3_count_UID...\n",
      "Create mac1_count_UID...\n",
      "Create mac2_count_UID...\n",
      "Create ip1_count_UID...\n",
      "Create ip2_count_UID...\n",
      "Create wifi_count_UID...\n",
      "Create geo_code_count_UID...\n",
      "Create ip1_sub_count_UID...\n",
      "Create ip2_sub_count_UID...\n",
      "Create hour_count_UID...\n",
      "Create day_frequency...\n",
      "Feature Creation Done\n",
      "Feature Creation Start\n",
      "Create channel_nunique...\n",
      "Create day_nunique...\n",
      "Create time_nunique...\n",
      "Create trans_amt_nunique...\n",
      "Create amt_src1_nunique...\n",
      "Create merchant_nunique...\n",
      "Create code1_nunique...\n",
      "Create code2_nunique...\n",
      "Create trans_type1_nunique...\n",
      "Create acc_id1_nunique...\n",
      "Create device_code1_nunique...\n",
      "Create device_code2_nunique...\n",
      "Create device_code3_nunique...\n",
      "Create device1_nunique...\n",
      "Create device2_nunique...\n",
      "Create mac1_nunique...\n",
      "Create ip1_nunique...\n",
      "Create bal_nunique...\n",
      "Create amt_src2_nunique...\n",
      "Create acc_id2_nunique...\n",
      "Create acc_id3_nunique...\n",
      "Create geo_code_nunique...\n",
      "Create trans_type2_nunique...\n",
      "Create market_code_nunique...\n",
      "Create market_type_nunique...\n",
      "Create ip1_sub_nunique...\n",
      "Create hour_nunique...\n",
      "Create channel_nunique_UID and channel_count_UID...\n",
      "Create day_nunique_UID and day_count_UID...\n",
      "Create time_nunique_UID and time_count_UID...\n",
      "Create trans_amt_nunique_UID and trans_amt_count_UID...\n",
      "Create amt_src1_nunique_UID and amt_src1_count_UID...\n",
      "Create merchant_nunique_UID and merchant_count_UID...\n",
      "Create code1_nunique_UID and code1_count_UID...\n",
      "Create code2_nunique_UID and code2_count_UID...\n",
      "Create trans_type1_nunique_UID and trans_type1_count_UID...\n",
      "Create acc_id1_nunique_UID and acc_id1_count_UID...\n",
      "Create device_code1_nunique_UID and device_code1_count_UID...\n",
      "Create device_code2_nunique_UID and device_code2_count_UID...\n",
      "Create device_code3_nunique_UID and device_code3_count_UID...\n",
      "Create device1_nunique_UID and device1_count_UID...\n",
      "Create device2_nunique_UID and device2_count_UID...\n",
      "Create mac1_nunique_UID and mac1_count_UID...\n",
      "Create ip1_nunique_UID and ip1_count_UID...\n",
      "Create bal_nunique_UID and bal_count_UID...\n",
      "Create amt_src2_nunique_UID and amt_src2_count_UID...\n",
      "Create acc_id2_nunique_UID and acc_id2_count_UID...\n",
      "Create acc_id3_nunique_UID and acc_id3_count_UID...\n",
      "Create geo_code_nunique_UID and geo_code_count_UID...\n",
      "Create trans_type2_nunique_UID and trans_type2_count_UID...\n",
      "Create market_code_nunique_UID and market_code_count_UID...\n",
      "Create market_type_nunique_UID and market_type_count_UID...\n",
      "Create ip1_sub_nunique_UID and ip1_sub_count_UID...\n",
      "Create hour_nunique_UID and hour_count_UID...\n",
      "Create channel_nunique_UID...\n",
      "Create day_nunique_UID...\n",
      "Create time_nunique_UID...\n",
      "Create trans_amt_nunique_UID...\n",
      "Create amt_src1_nunique_UID...\n",
      "Create merchant_nunique_UID...\n",
      "Create code1_nunique_UID...\n",
      "Create code2_nunique_UID...\n",
      "Create trans_type1_nunique_UID...\n",
      "Create acc_id1_nunique_UID...\n",
      "Create device_code1_nunique_UID...\n",
      "Create device_code2_nunique_UID...\n",
      "Create device_code3_nunique_UID...\n",
      "Create device1_nunique_UID...\n",
      "Create device2_nunique_UID...\n",
      "Create mac1_nunique_UID...\n",
      "Create ip1_nunique_UID...\n",
      "Create bal_nunique_UID...\n",
      "Create amt_src2_nunique_UID...\n",
      "Create acc_id2_nunique_UID...\n",
      "Create acc_id3_nunique_UID...\n",
      "Create geo_code_nunique_UID...\n",
      "Create trans_type2_nunique_UID...\n",
      "Create market_code_nunique_UID...\n",
      "Create market_type_nunique_UID...\n",
      "Create ip1_sub_nunique_UID...\n",
      "Create hour_nunique_UID...\n",
      "Create channel_count_UID...\n",
      "Create day_count_UID...\n",
      "Create time_count_UID...\n",
      "Create trans_amt_count_UID...\n",
      "Create amt_src1_count_UID...\n",
      "Create merchant_count_UID...\n",
      "Create code1_count_UID...\n",
      "Create code2_count_UID...\n",
      "Create trans_type1_count_UID...\n",
      "Create acc_id1_count_UID...\n",
      "Create device_code1_count_UID...\n",
      "Create device_code2_count_UID...\n",
      "Create device_code3_count_UID...\n",
      "Create device1_count_UID...\n",
      "Create device2_count_UID...\n",
      "Create mac1_count_UID...\n",
      "Create ip1_count_UID...\n",
      "Create bal_count_UID...\n",
      "Create amt_src2_count_UID...\n",
      "Create acc_id2_count_UID...\n",
      "Create acc_id3_count_UID...\n",
      "Create geo_code_count_UID...\n",
      "Create trans_type2_count_UID...\n",
      "Create market_code_count_UID...\n",
      "Create market_type_count_UID...\n",
      "Create ip1_sub_count_UID...\n",
      "Create hour_count_UID...\n",
      "Create day_frequency...\n",
      "Feature Creation Done\n",
      "Feature Selection Start\n",
      "Feature Selection Done\n",
      "Feature Engineering Done\n",
      "Model Optimizatioin Start\n",
      "Optimize param {'C': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10]} ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize param {'class_weight': [None, 'balanced']} ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize param {'max_iter': [100, 300, 500, 1000]} ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize param {'penalty': ['l1', 'l2']} ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize param {'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']} ...\n",
      "Model Optimizatioin Done\n",
      "Model Optimizatioin Start\n",
      "Optimize param {'n_estimators': [100, 300, 500, 1000]} ...\n",
      "Optimize param {'learning_rate': [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0]} ...\n",
      "Optimize param {'max_features': [None, 'log2', 'sqrt']} ...\n",
      "Optimize param {'max_depth': [3, 5, 7, 9]} ...\n",
      "Optimize param {'min_samples_split': [2, 4, 6, 8]} ...\n",
      "Optimize param {'min_samples_leaf': [1, 3, 5, 7]} ...\n",
      "Model Optimizatioin Done\n",
      "Model Optimizatioin Start\n",
      "Optimize param {'learning_rate': [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1]} ...\n",
      "Optimize param {'n_estimators': [100, 300, 500, 1000]} ...\n",
      "Optimize param {'max_depth': range(3, 10, 2)} ...\n",
      "Optimize param {'min_child_weight': range(1, 6, 2)} ...\n",
      "Optimize param {'gamma': [0.0, 0.1, 0.2, 0.3, 0.4]} ...\n",
      "Optimize param {'subsample': [0.6, 0.7, 0.8, 0.9]} ...\n",
      "Optimize param {'colsample_bytree': [0.6, 0.7, 0.8, 0.9]} ...\n",
      "Optimize param {'reg_alpha': [1e-05, 0.01, 0.1, 1, 100]} ...\n",
      "Model Optimizatioin Done\n",
      "Model Evaluation Start\n",
      "Compute roc_auc_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   43.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   43.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute tpr_weight_score...\n",
      "Split data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:422: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\sklearn\\utils\\optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\sklearn\\utils\\optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:422: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\sklearn\\utils\\optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Done\n",
      "Record Start\n",
      "Record Done\n",
      "Model Evaluation Start\n",
      "Compute roc_auc_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   27.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   27.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute tpr_weight_score...\n",
      "Split data...\n",
      "Split data...\n",
      "Split data...\n",
      "Model Evaluation Done\n",
      "Record Start\n",
      "Record Done\n",
      "Model Evaluation Start\n",
      "Compute roc_auc_score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   14.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   14.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute tpr_weight_score...\n",
      "Split data...\n",
      "Split data...\n",
      "Split data...\n",
      "Model Evaluation Done\n",
      "Record Start\n",
      "Record Done\n",
      "Fit Predict Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:462: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:422: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\scipy\\optimize\\linesearch.py:313: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "g:\\python\\lib\\site-packages\\sklearn\\utils\\optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Predict Done\n",
      "Fit Predict Start\n",
      "Fit Predict Done\n",
      "Fit Predict Start\n",
      "Fit Predict Done\n",
      "t_fit_predict:  80.91845035552979\n",
      "Ensembling Start\n",
      "Ensembling Done\n",
      "Submit Start\n",
      "Submit Done\n",
      "Submit Start\n",
      "Submit Done\n",
      "Submit Start\n",
      "Submit Done\n",
      "Submit Start\n",
      "Submit Done\n",
      "t_get_data:  28.466918230056763\n",
      "t_data_preprocessing:  10.8610098361969\n",
      "t_feature_engineering:  319.28134751319885\n",
      "t_model_optimization:  568.4811298847198\n",
      "t_model_evaluation:  257.55098581314087\n",
      "t_ensembling:  0.0019953250885009766\n",
      "t_submit:  0.46979618072509766\n",
      "total_time:  1266.0316331386566\n"
     ]
    }
   ],
   "source": [
    "# Get Data\n",
    "t_get_data_start = time.time()\n",
    "operation_train, transaction_train, tag_train, operation_test, transaction_test, tag_test = GetData(\"../Data/\")\n",
    "t_get_data_end = time.time()\n",
    "\n",
    "# Data Preprocessing\n",
    "t_data_preprocessing_start = time.time()\n",
    "operation_train = DataPreprocessing(operation_train)\n",
    "transaction_train = DataPreprocessing(transaction_train)\n",
    "operation_test = DataPreprocessing(operation_test)\n",
    "transaction_test = DataPreprocessing(transaction_test)\n",
    "t_data_preprocessing_end = time.time()\n",
    "\n",
    "# # Data Exploration\n",
    "# t_data_exploration_start = time.time()\n",
    "# DataExploration(operation_train, tag_train)\n",
    "# DataExploration(transaciont_train, tag_train)\n",
    "# DataExploration(opearation_test)\n",
    "# DataExploration(transaction_test)\n",
    "# t_data_exploration_end = time.time()\n",
    "\n",
    "# Feature Engineering\n",
    "t_feature_engineering_start = time.time()\n",
    "x_train, x_test = FeatureEngineering(operation_train, transaction_train, tag_train, operation_test, transaction_test, tag_test)\n",
    "t_feature_engineering_end = time.time()\n",
    "\n",
    "# Model Optimization\n",
    "t_model_optimization_start = time.time()\n",
    "lr_params = [{\"C\": [0.01, 0.03, 0.1, 0.3, 1.0, 3.0, 10]}, \n",
    "             {\"class_weight\": [None, \"balanced\"]}, \n",
    "             {\"max_iter\": [100, 300, 500, 1000]}, \n",
    "             {\"penalty\": [\"l1\", \"l2\"]},\n",
    "             {\"solver\": ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}]\n",
    "lr_best_params = ModelOptimization(LogisticRegression(), lr_params, x_train, tag_train[\"Tag\"])\n",
    "\n",
    "gbdt_params = [{\"n_estimators\": [100, 300, 500, 1000]}, \n",
    "               {\"learning_rate\": [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0]}, \n",
    "               {\"max_features\": [None, \"log2\", \"sqrt\"]}, \n",
    "               {\"max_depth\": [3, 5, 7, 9]}, \n",
    "               {\"min_samples_split\": [2, 4, 6, 8]}, \n",
    "               {\"min_samples_leaf\": [1, 3, 5, 7]}]\n",
    "gbdt_best_params = ModelOptimization(GradientBoostingClassifier(), gbdt_params, x_train, tag_train[\"Tag\"])\n",
    "\n",
    "xgb_params = [{\"learning_rate\": [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1]}, \n",
    "              {\"n_estimators\": [100, 300, 500, 1000]}, \n",
    "              {\"max_depth\": range(3,10,2)}, \n",
    "              {\"min_child_weight\": range(1,6,2)}, \n",
    "              {\"gamma\": [i/10.0 for i in range(0,5)]}, \n",
    "              {\"subsample\": [i/10.0 for i in range(6,10)]},\n",
    "              {\"colsample_bytree\": [i/10.0 for i in range(6,10)]}, \n",
    "              {\"reg_alpha\": [1e-5, 1e-2, 0.1, 1, 100]}]\n",
    "xgb_best_params = ModelOptimization(XGBClassifier(), xgb_params, x_train, tag_train[\"Tag\"])\n",
    "t_model_optimization_end = time.time()\n",
    "\n",
    "# Model Evaluation\n",
    "t_model_evaluation_start = time.time()\n",
    "lr = LogisticRegression(C = 10, class_weight = \"balanced\", max_iter = 1000, penalty = \"l2\", solver = \"newton-cg\")\n",
    "lr_roc_auc_score, lr_tpr_weight_score = ModelEvaluation(lr, x_train, tag_train[\"Tag\"])\n",
    "Record(x_train, lr, lr_roc_auc_score, lr_tpr_weight_score)\n",
    "\n",
    "gbdt = GradientBoostingClassifier(n_estimators = 1000, learning_rate = 0.3, max_features = None, \n",
    "                                  max_depth = 3, min_samples_split = 4, min_samples_leaf = 7)\n",
    "gbdt_roc_auc_score, gbdt_tpr_weight_score = ModelEvaluation(gbdt, x_train, tag_train[\"Tag\"])\n",
    "Record(x_train, gbdt, gbdt_roc_auc_score, gbdt_tpr_weight_score)\n",
    "\n",
    "xgb = XGBClassifier(n_estimators = 300, learning_rate = 0.3, max_depth = 5, min_child_weight = 1, \n",
    "                    gamma = 0.1, subsample = 0.8, colsample_bytree = 0.8, reg_alpha = 1e-05)\n",
    "xgb_roc_auc_score, xgb_tpr_weight_score = ModelEvaluation(xgb, x_train, tag_train[\"Tag\"])\n",
    "Record(x_train, xgb, xgb_roc_auc_score, xgb_tpr_weight_score)\n",
    "t_model_evaluation_end = time.time()\n",
    "\n",
    "# Fit and Predict\n",
    "t_fit_predict_start = time.time()\n",
    "lr_y_pred = FitPredict(lr, x_train, tag_train[\"Tag\"], x_test)\n",
    "gbdt_y_pred = FitPredict(gbdt, x_train, tag_train[\"Tag\"], x_test)\n",
    "xgb_y_pred = FitPredict(xgb, x_train, tag_train[\"Tag\"], x_test)\n",
    "t_fit_predict_end = time.time()\n",
    "print(\"t_fit_predict: \", t_fit_predict_end - t_fit_predict_start)\n",
    "\n",
    "# Ensembling\n",
    "t_ensembling_start = time.time()\n",
    "y_pred_list = [lr_y_pred, gbdt_y_pred, xgb_y_pred]\n",
    "ensembling_y_pred = Ensembling(y_pred_list, tag_test)\n",
    "t_ensembling_end = time.time()\n",
    "\n",
    "# Submit\n",
    "t_submit_start = time.time()\n",
    "lr_submission = Submit(lr_y_pred, tag_test)\n",
    "gbdt_submission = Submit(gbdt_y_pred, tag_test)\n",
    "xgb_submission = Submit(xgb_y_pred, tag_test)\n",
    "ensembling_submission = Submit(ensembling_y_pred, tag_test)\n",
    "lr_submission.to_csv(\"../Submission/lr_submission.csv\", index = False)\n",
    "gbdt_submission.to_csv(\"../Submission/gbdt_submission.csv\", index = False)\n",
    "xgb_submission.to_csv(\"../Submission/xgb_submission.csv\", index = False)\n",
    "ensembling_submission.to_csv(\"../Submission/ensembling_submission.csv\", index = False)\n",
    "t_submit_end = time.time()\n",
    "\n",
    "print(\"t_get_data: \", t_get_data_end - t_get_data_start)\n",
    "print(\"t_data_preprocessing: \", t_data_preprocessing_end - t_data_preprocessing_start)\n",
    "# print(\"t_data_exploration: \", t_data_exploration_end - t_data_exploration_start)\n",
    "print(\"t_feature_engineering: \", t_feature_engineering_end - t_feature_engineering_start)\n",
    "print(\"t_model_optimization: \", t_model_optimization_end - t_model_optimization_start)\n",
    "print(\"t_model_evaluation: \", t_model_evaluation_end - t_model_evaluation_start)\n",
    "print(\"t_ensembling: \", t_ensembling_end - t_ensembling_start)\n",
    "print(\"t_submit: \", t_submit_end - t_submit_start)\n",
    "print(\"total_time: \", t_submit_end - t_get_data_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
